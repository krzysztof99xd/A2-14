{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Assignment 2, Natural Language Processing, Group 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and choose only the articles where NVIDIA word appears in the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id ticker                                              title  \\\n",
      "24      221539    NIO  A Central Bank War Just Started And Its Good F...   \n",
      "32      221547    NIO         6 Stocks To Watch  Nivida Could Be Falling   \n",
      "57      221572    NIO  Stocks   Dow Drops Nearly 400 Points as Apple ...   \n",
      "78      221593   UBER  The Zacks Analyst Blog Highlights  Advanced Mi...   \n",
      "82      221597   UBER                     The Best Of CES 2020  Revised    \n",
      "...        ...    ...                                                ...   \n",
      "221141  442657    AMD    Here s Why Nvidia  NVDA  Stock Is Gaining Today   \n",
      "221166  442682    AMD      4 Stocks To Watch Today  ATW  CWEI  MXL  SLCA   \n",
      "221189  442705    AMD  Here s What The Buy Side Expects From AMD Thur...   \n",
      "221468  442984      T  Zacks com Featured Highlights  AT T  Nu Skin E...   \n",
      "221471  442987      T  5 Dividend Growth Stocks To Sail Through Uncer...   \n",
      "\n",
      "       category                                            content  \\\n",
      "24      opinion  ECB Effects\\nThe move in the euro was huge  fa...   \n",
      "32      opinion  6 Stocks To Watch  March 6 Trading Session\\nSt...   \n",
      "57         news  Investing com   A rout in Apple and Facebook  ...   \n",
      "78      opinion  For Immediate ReleaseChicago  IL   January 13 ...   \n",
      "82      opinion  With 4 500 companies bringing their innovation...   \n",
      "...         ...                                                ...   \n",
      "221141  opinion  Shares of Nvidia   NASDAQ NVDA   are up nearly...   \n",
      "221166  opinion  It was a pretty good start to the week on Mond...   \n",
      "221189  opinion  Advanced Micro Devices Inc   NYSE AMD  is set ...   \n",
      "221468  opinion  For Immediate Release\\n\\nChicago  IL   July 22...   \n",
      "221471  opinion  With uncertainty ruling the markets since the ...   \n",
      "\n",
      "       release_date                   provider  \\\n",
      "24       2019-03-07             Michael Kramer   \n",
      "32       2019-03-06             Michael Kramer   \n",
      "57       2018-11-19              Investing.com   \n",
      "78       2020-01-12  Zacks Investment Research   \n",
      "82       2020-01-16  Zacks Investment Research   \n",
      "...             ...                        ...   \n",
      "221141   2016-09-27  Zacks Investment Research   \n",
      "221166   2016-05-17                Harry Boxer   \n",
      "221189   2014-04-17                   Estimize   \n",
      "221468   2016-07-21  Zacks Investment Research   \n",
      "221471   2016-07-20  Zacks Investment Research   \n",
      "\n",
      "                                                      url  article_id  \n",
      "24      https://www.investing.com/analysis/a-central-b...   200395687  \n",
      "32      https://www.investing.com/analysis/6-stocks-to...   200394931  \n",
      "57      https://www.investing.com/news/stock-market-ne...     1694042  \n",
      "78      https://www.investing.com/analysis/the-zacks-a...   200498277  \n",
      "82      https://www.investing.com/analysis/the-best-of...   200499164  \n",
      "...                                                   ...         ...  \n",
      "221141  https://www.investing.com/analysis/here's-why-...   200155860  \n",
      "221166  https://www.investing.com/analysis/atw,-cwei,-...   200130262  \n",
      "221189  https://www.investing.com/analysis/hereâ€™s-what...      209915  \n",
      "221468  https://www.investing.com/analysis/zacks.com-f...   200143537  \n",
      "221471  https://www.investing.com/analysis/5-dividend-...   200143306  \n",
      "\n",
      "[3400 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('us_equities_news_dataset.csv')\n",
    "\n",
    "# Filter rows where 'content' column contains the word 'NVIDIA'\n",
    "nvidia_rows = df[df['content'].str.contains('NVIDIA', case=False, na=False)]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(nvidia_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'ticker', 'title', 'category', 'content', 'release_date',\n",
      "       'provider', 'url', 'article_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove the specified columns\n",
    "nvidia_rows = nvidia_rows.drop(['id', 'ticker', 'url', 'release_date'], axis=1)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "nvidia_rows.to_csv('nvidia_rows_news_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text_data 100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m text_data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of text_data \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_texts([x], \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     44\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia_rows_news_dataset_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m processed_articles \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[89], line 43\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     40\u001b[0m text_data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of text_data \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_texts([x], \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     44\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia_rows_news_dataset_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m processed_articles \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[89], line 16\u001b[0m, in \u001b[0;36mpreprocess_texts\u001b[1;34m(texts, n)\u001b[0m\n\u001b[0;32m     14\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# lowercasing, keep text only, remove accents, tokenization\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, unidecode(texts\u001b[38;5;241m.\u001b[39mlower())))]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# stopword removal\u001b[39;00m\n\u001b[0;32m     18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_texts(texts, n=0):\n",
    "    processed_texts = []\n",
    "    # lowercasing, keep text only, remove accents, tokenization\n",
    "    tokens = [word for word in word_tokenize(re.sub(r'[^a-zA-Z\\s]', '', unidecode(texts.lower())))]\n",
    "    # stopword removal\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # remove short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "        \n",
    "    # Apply stemming to each token\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    processed_texts.append(stemmed_tokens)\n",
    "\n",
    "    # remove top-n% and bottom-n% words (optional)\n",
    "    if n > 0:\n",
    "        word_freq = Counter([word for sentence in processed_texts for word in sentence])\n",
    "        top_n = set([word for word, _ in word_freq.most_common(int(n/100*len(word_freq)))])\n",
    "        bottom_n = set([word for word, _ in word_freq.most_common()[:-int(n/100*len(word_freq))-1:-1]])\n",
    "        processed_texts = [[word for word in sentence if word not in top_n and word not in bottom_n] for sentence in processed_texts]    \n",
    "    return processed_texts\n",
    "\n",
    "# Example usage\n",
    "FILE_PATH = 'nvidia_rows_news_dataset.csv'\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# df = df.head(100)\n",
    "text_data = df['content'].tolist()\n",
    "print(f\"Length of text_data {len(text_data)}\")\n",
    "\n",
    "# df['processed_content'] = df['content'].apply(lambda x: preprocess_texts([x], 0))\n",
    "# df.to_csv(\"nvidia_rows_news_dataset_processed.csv\", index=False)\n",
    "\n",
    "processed_articles = []\n",
    "print(f\"Length of processed_articles before processing {len(processed_articles)}\")\n",
    "\n",
    "for single_article in text_data:        \n",
    "    processed_article = preprocess_texts(single_article, 0)\n",
    "    processed_articles.append(processed_article)\n",
    "\n",
    "print(f\"Length of processed_articles {len(processed_articles)}\")\n",
    "print(\"Finished with processing the data\")\n",
    "print(\"Writing to processed_data.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(\"processed_data.txt\", \"w\") as file:\n",
    "    # Iterate through the processed_articles and write each article's tokens on a new line\n",
    "    for article in processed_articles:\n",
    "        # Ensure that article is a flat list of strings\n",
    "        if isinstance(article, list):\n",
    "            # Join the words (tokens) of each article with a space and write them as one line\n",
    "            file.write(\" \".join(str(token) for token in article) + \"\\n\")\n",
    "        \n",
    "        # Write an empty line to separate articles\n",
    "        file.write(\"\\n\")\n",
    "# print(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"compani\" + 0.013*\"technolog\" + 0.013*\"nasdaq\" + 0.011*\"zack\" + 0.011*\"market\"')\n",
      "(1, '0.026*\"drive\" + 0.022*\"car\" + 0.018*\"nvidia\" + 0.017*\"vehicl\" + 0.017*\"self\"')\n",
      "(2, '0.033*\"quarter\" + 0.031*\"year\" + 0.022*\"revenu\" + 0.021*\"zack\" + 0.021*\"compani\"')\n",
      "(3, '0.012*\"market\" + 0.011*\"trade\" + 0.009*\"week\" + 0.008*\"year\" + 0.007*\"china\"')\n",
      "(4, '0.025*\"stock\" + 0.022*\"earn\" + 0.020*\"year\" + 0.020*\"zack\" + 0.016*\"compani\"')\n",
      "(5, '0.030*\"zack\" + 0.020*\"invest\" + 0.017*\"analyst\" + 0.015*\"stock\" + 0.015*\"research\"')\n",
      "(6, '0.035*\"nvidia\" + 0.029*\"amd\" + 0.027*\"game\" + 0.021*\"gpu\" + 0.014*\"graphic\"')\n",
      "(7, '0.039*\"stock\" + 0.035*\"trade\" + 0.032*\"invest\" + 0.023*\"nvidia\" + 0.021*\"nasdaq\"')\n",
      "(8, '0.055*\"inc\" + 0.039*\"nasdaq\" + 0.026*\"nyse\" + 0.016*\"biotech\" + 0.015*\"share\"')\n",
      "(9, '0.011*\"simo\" + 0.010*\"motion\" + 0.010*\"ssd\" + 0.007*\"silicon\" + 0.006*\"control\"')\n",
      "(10, '0.011*\"appl\" + 0.009*\"googl\" + 0.008*\"compani\" + 0.006*\"new\" + 0.006*\"also\"')\n",
      "(11, '0.019*\"stock\" + 0.013*\"etf\" + 0.012*\"nasdaq\" + 0.011*\"market\" + 0.010*\"week\"')\n",
      "(12, '0.046*\"nasdaq\" + 0.024*\"nyse\" + 0.023*\"percent\" + 0.020*\"inc\" + 0.017*\"stock\"')\n",
      "(13, '0.019*\"compani\" + 0.017*\"year\" + 0.016*\"earn\" + 0.016*\"revenu\" + 0.015*\"growth\"')\n",
      "(14, '0.048*\"bitcoin\" + 0.034*\"cryptocurr\" + 0.021*\"mine\" + 0.016*\"blockchain\" + 0.011*\"crypto\"')\n",
      "0.42464476000488893\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import re\n",
    "\n",
    "read_processed_data = []  # Initialize an empty list\n",
    "current_document = []      # Temporary list to store tokens for the current document\n",
    "\n",
    "with open(\"processed_data.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        # Check if the line is empty (i.e., it's a document separator)\n",
    "        if line.strip() == \"\":\n",
    "            # If we have tokens in the current document, add them to the list\n",
    "            if current_document:\n",
    "                read_processed_data.append(current_document)\n",
    "                current_document = []  # Reset for the next document\n",
    "        else:\n",
    "            # Remove any unwanted characters (like brackets or quotes) from the line\n",
    "            cleaned_line = line.strip().replace('[', '').replace(']', '')\n",
    "            \n",
    "            # Split the line into words\n",
    "            words = cleaned_line.split()\n",
    "            \n",
    "            # Clean each word to remove punctuation (like commas and single quotes)\n",
    "            cleaned_words = [re.sub(r\"[',]\", '', word) for word in words]\n",
    "            \n",
    "            # Add the cleaned words to the current document\n",
    "            current_document.extend(cleaned_words)\n",
    "\n",
    "    # After the loop, add the last document (if any) to the list\n",
    "    if current_document:\n",
    "        read_processed_data.append(current_document)\n",
    "\n",
    "# # Check the structure of the read_processed_data\n",
    "# for i, doc in enumerate(read_processed_data[:2]):  # Print the first two documents for inspection\n",
    "#     print(f\"Document {i+1}: {doc}\")\n",
    "\n",
    "# Create a dictionary and corpus from the preprocessed data\n",
    "dictionary = corpora.Dictionary(read_processed_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in read_processed_data]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, random_state=100, passes=10)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "#Get coherence score\n",
    "print(CoherenceModel(model=lda_model, texts=read_processed_data, dictionary=dictionary, coherence='c_v').get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BERTTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bert Topic code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLSA-W Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLSA-W Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
